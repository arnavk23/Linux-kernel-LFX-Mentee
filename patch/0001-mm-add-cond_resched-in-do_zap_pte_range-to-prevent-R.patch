From 9ff3a7b41518c479dfc025cdd999ea15ab2b938e Mon Sep 17 00:00:00 2001
From: Arnav Kapoor <kapoorarnav43@gmail.com>
Date: Sun, 22 Jun 2025 12:34:25 +0530
Subject: [PATCH] mm: add cond_resched() in do_zap_pte_range to prevent RCU
 stalls

Large VMA unmaps can loop for extended periods in do_zap_pte_range(),
especially when skipping or processing many PTEs. This can lead to
soft lockups and RCU stall warnings under heavy memory pressure.

Add cond_resched() to allow the CPU to yield and avoid triggering
such stalls.

Reported-by: syzbot+2642f347f7309b4880dc@syzkaller.appspotmail.com
Signed-off-by: Arnav Kapoor <kapoorarnav43@gmail.com>
---
 mm/memory.c | 831 ++++++++++++++++++++++++++++------------------------
 1 file changed, 444 insertions(+), 387 deletions(-)

diff --git a/mm/memory.c b/mm/memory.c
index ba3ea0a82f7f..5cd88c86b6ea 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -120,9 +120,9 @@ static __always_inline bool vmf_orig_pte_uffd_wp(struct vm_fault *vmf)
  */
 int randomize_va_space __read_mostly =
 #ifdef CONFIG_COMPAT_BRK
-					1;
+	1;
 #else
-					2;
+	2;
 #endif
 
 #ifndef arch_wants_old_prefaulted_pte
@@ -178,8 +178,8 @@ static void free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,
 }
 
 static inline void free_pmd_range(struct mmu_gather *tlb, pud_t *pud,
-				unsigned long addr, unsigned long end,
-				unsigned long floor, unsigned long ceiling)
+				  unsigned long addr, unsigned long end,
+				  unsigned long floor, unsigned long ceiling)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -212,8 +212,8 @@ static inline void free_pmd_range(struct mmu_gather *tlb, pud_t *pud,
 }
 
 static inline void free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,
-				unsigned long addr, unsigned long end,
-				unsigned long floor, unsigned long ceiling)
+				  unsigned long addr, unsigned long end,
+				  unsigned long floor, unsigned long ceiling)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -246,8 +246,8 @@ static inline void free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,
 }
 
 static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,
-				unsigned long addr, unsigned long end,
-				unsigned long floor, unsigned long ceiling)
+				  unsigned long addr, unsigned long end,
+				  unsigned long floor, unsigned long ceiling)
 {
 	p4d_t *p4d;
 	unsigned long next;
@@ -281,9 +281,9 @@ static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,
 /*
  * This function frees user-level page tables of a process.
  */
-void free_pgd_range(struct mmu_gather *tlb,
-			unsigned long addr, unsigned long end,
-			unsigned long floor, unsigned long ceiling)
+void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
+		    unsigned long end, unsigned long floor,
+		    unsigned long ceiling)
 {
 	pgd_t *pgd;
 	unsigned long next;
@@ -371,8 +371,8 @@ void free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,
 
 		if (is_vm_hugetlb_page(vma)) {
 			unlink_file_vma(vma);
-			hugetlb_free_pgd_range(tlb, addr, vma->vm_end,
-				floor, next ? next->vm_start : ceiling);
+			hugetlb_free_pgd_range(tlb, addr, vma->vm_end, floor,
+					       next ? next->vm_start : ceiling);
 		} else {
 			unlink_file_vma_batch_init(&vb);
 			unlink_file_vma_batch_add(&vb, vma);
@@ -380,8 +380,9 @@ void free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,
 			/*
 			 * Optimization: gather nearby vmas into one call down
 			 */
-			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
-			       && !is_vm_hugetlb_page(next)) {
+			while (next &&
+			       next->vm_start <= vma->vm_end + PMD_SIZE &&
+			       !is_vm_hugetlb_page(next)) {
 				vma = next;
 				next = mas_find(mas, ceiling - 1);
 				if (unlikely(xa_is_zero(next)))
@@ -392,8 +393,8 @@ void free_pgtables(struct mmu_gather *tlb, struct ma_state *mas,
 				unlink_file_vma_batch_add(&vb, vma);
 			}
 			unlink_file_vma_batch_final(&vb);
-			free_pgd_range(tlb, addr, vma->vm_end,
-				floor, next ? next->vm_start : ceiling);
+			free_pgd_range(tlb, addr, vma->vm_end, floor,
+				       next ? next->vm_start : ceiling);
 		}
 		vma = next;
 	} while (vma);
@@ -403,7 +404,7 @@ void pmd_install(struct mm_struct *mm, pmd_t *pmd, pgtable_t *pte)
 {
 	spinlock_t *ptl = pmd_lock(mm, pmd);
 
-	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
+	if (likely(pmd_none(*pmd))) { /* Has another populated it ? */
 		mm_inc_nr_ptes(mm);
 		/*
 		 * Ensure all pte setup (eg. pte page lock and page clearing) are
@@ -444,7 +445,7 @@ int __pte_alloc_kernel(pmd_t *pmd)
 		return -ENOMEM;
 
 	spin_lock(&init_mm.page_table_lock);
-	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
+	if (likely(pmd_none(*pmd))) { /* Has another populated it ? */
 		smp_wmb(); /* See comment in pmd_install() */
 		pmd_populate_kernel(&init_mm, pmd, new);
 		new = NULL;
@@ -512,14 +513,13 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 	index = linear_page_index(vma, addr);
 
 	pr_alert("BUG: Bad page map in process %s  pte:%08llx pmd:%08llx\n",
-		 current->comm,
-		 (long long)pte_val(pte), (long long)pmd_val(*pmd));
+		 current->comm, (long long)pte_val(pte),
+		 (long long)pmd_val(*pmd));
 	if (page)
 		dump_page(page, "bad pte");
 	pr_alert("addr:%px vm_flags:%08lx anon_vma:%px mapping:%px index:%lx\n",
 		 (void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);
-	pr_alert("file:%pD fault:%ps mmap:%ps read_folio:%ps\n",
-		 vma->vm_file,
+	pr_alert("file:%pD fault:%ps mmap:%ps read_folio:%ps\n", vma->vm_file,
 		 vma->vm_ops ? vma->vm_ops->fault : NULL,
 		 vma->vm_file ? vma->vm_file->f_op->mmap : NULL,
 		 mapping ? mapping->a_ops->read_folio : NULL);
@@ -587,7 +587,7 @@ struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 		if (is_zero_pfn(pfn))
 			return NULL;
 		if (pte_devmap(pte))
-		/*
+			/*
 		 * NOTE: New users of ZONE_DEVICE will not set pte_devmap()
 		 * and will have refcounts incremented on their struct pages
 		 * when they are inserted into PTEs, thus they are safe to
@@ -603,7 +603,7 @@ struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 
 	/* !CONFIG_ARCH_HAS_PTE_SPECIAL case follows: */
 
-	if (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {
+	if (unlikely(vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))) {
 		if (vma->vm_flags & VM_MIXEDMAP) {
 			if (!pfn_valid(pfn))
 				return NULL;
@@ -639,7 +639,7 @@ struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 }
 
 struct folio *vm_normal_folio(struct vm_area_struct *vma, unsigned long addr,
-			    pte_t pte)
+			      pte_t pte)
 {
 	struct page *page = vm_normal_page(vma, addr, pte);
 
@@ -658,7 +658,7 @@ struct page *vm_normal_page_pmd(struct vm_area_struct *vma, unsigned long addr,
 	if (unlikely(pmd_special(pmd)))
 		return NULL;
 
-	if (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {
+	if (unlikely(vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))) {
 		if (vma->vm_flags & VM_MIXEDMAP) {
 			if (!pfn_valid(pfn))
 				return NULL;
@@ -726,8 +726,9 @@ struct folio *vm_normal_folio_pmd(struct vm_area_struct *vma,
  * must use MMU notifiers to sync against any concurrent changes.
  */
 static void restore_exclusive_pte(struct vm_area_struct *vma,
-		struct folio *folio, struct page *page, unsigned long address,
-		pte_t *ptep, pte_t orig_pte)
+				  struct folio *folio, struct page *page,
+				  unsigned long address, pte_t *ptep,
+				  pte_t orig_pte)
 {
 	pte_t pte;
 
@@ -760,7 +761,8 @@ static void restore_exclusive_pte(struct vm_area_struct *vma,
  * sleeping.
  */
 static int try_restore_exclusive_pte(struct vm_area_struct *vma,
-		unsigned long addr, pte_t *ptep, pte_t orig_pte)
+				     unsigned long addr, pte_t *ptep,
+				     pte_t orig_pte)
 {
 	struct page *page = pfn_swap_entry_to_page(pte_to_swp_entry(orig_pte));
 	struct folio *folio = page_folio(page);
@@ -780,10 +782,12 @@ static int try_restore_exclusive_pte(struct vm_area_struct *vma,
  * covered by this vma.
  */
 
-static unsigned long
-copy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
-		pte_t *dst_pte, pte_t *src_pte, struct vm_area_struct *dst_vma,
-		struct vm_area_struct *src_vma, unsigned long addr, int *rss)
+static unsigned long copy_nonpresent_pte(struct mm_struct *dst_mm,
+					 struct mm_struct *src_mm,
+					 pte_t *dst_pte, pte_t *src_pte,
+					 struct vm_area_struct *dst_vma,
+					 struct vm_area_struct *src_vma,
+					 unsigned long addr, int *rss)
 {
 	unsigned long vm_flags = dst_vma->vm_flags;
 	pte_t orig_pte = ptep_get(src_pte);
@@ -800,8 +804,7 @@ copy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		if (unlikely(list_empty(&dst_mm->mmlist))) {
 			spin_lock(&mmlist_lock);
 			if (list_empty(&dst_mm->mmlist))
-				list_add(&dst_mm->mmlist,
-						&src_mm->mmlist);
+				list_add(&dst_mm->mmlist, &src_mm->mmlist);
 			spin_unlock(&mmlist_lock);
 		}
 		/* Mark the swap entry as shared. */
@@ -816,14 +819,14 @@ copy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		rss[mm_counter(folio)]++;
 
 		if (!is_readable_migration_entry(entry) &&
-				is_cow_mapping(vm_flags)) {
+		    is_cow_mapping(vm_flags)) {
 			/*
 			 * COW mappings require pages in both parent and child
 			 * to be set to read. A previously exclusive entry is
 			 * now shared.
 			 */
 			entry = make_readable_migration_entry(
-							swp_offset(entry));
+				swp_offset(entry));
 			pte = swp_entry_to_pte(entry);
 			if (pte_swp_soft_dirty(orig_pte))
 				pte = pte_swp_mksoft_dirty(pte);
@@ -859,7 +862,7 @@ copy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		if (is_writable_device_private_entry(entry) &&
 		    is_cow_mapping(vm_flags)) {
 			entry = make_readable_device_private_entry(
-							swp_offset(entry));
+				swp_offset(entry));
 			pte = swp_entry_to_pte(entry);
 			if (pte_swp_uffd_wp(orig_pte))
 				pte = pte_swp_mkuffd_wp(pte);
@@ -902,10 +905,11 @@ copy_nonpresent_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
  * code know so that it can do so outside the page table
  * lock.
  */
-static inline int
-copy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
-		  pte_t *dst_pte, pte_t *src_pte, unsigned long addr, int *rss,
-		  struct folio **prealloc, struct page *page)
+static inline int copy_present_page(struct vm_area_struct *dst_vma,
+				    struct vm_area_struct *src_vma,
+				    pte_t *dst_pte, pte_t *src_pte,
+				    unsigned long addr, int *rss,
+				    struct folio **prealloc, struct page *page)
 {
 	struct folio *new_folio;
 	pte_t pte;
@@ -939,8 +943,10 @@ copy_present_page(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma
 }
 
 static __always_inline void __copy_present_ptes(struct vm_area_struct *dst_vma,
-		struct vm_area_struct *src_vma, pte_t *dst_pte, pte_t *src_pte,
-		pte_t pte, unsigned long addr, int nr)
+						struct vm_area_struct *src_vma,
+						pte_t *dst_pte, pte_t *src_pte,
+						pte_t pte, unsigned long addr,
+						int nr)
 {
 	struct mm_struct *src_mm = src_vma->vm_mm;
 
@@ -968,10 +974,11 @@ static __always_inline void __copy_present_ptes(struct vm_area_struct *dst_vma,
  * Returns -EAGAIN if one preallocated page is required to copy the next PTE.
  * Otherwise, returns the number of copied PTEs (at least 1).
  */
-static inline int
-copy_present_ptes(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
-		 pte_t *dst_pte, pte_t *src_pte, pte_t pte, unsigned long addr,
-		 int max_nr, int *rss, struct folio **prealloc)
+static inline int copy_present_ptes(struct vm_area_struct *dst_vma,
+				    struct vm_area_struct *src_vma,
+				    pte_t *dst_pte, pte_t *src_pte, pte_t pte,
+				    unsigned long addr, int max_nr, int *rss,
+				    struct folio **prealloc)
 {
 	struct page *page;
 	struct folio *folio;
@@ -1000,8 +1007,8 @@ copy_present_ptes(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma
 				     &any_writable, NULL, NULL);
 		folio_ref_add(folio, nr);
 		if (folio_test_anon(folio)) {
-			if (unlikely(folio_try_dup_anon_rmap_ptes(folio, page,
-								  nr, dst_vma, src_vma))) {
+			if (unlikely(folio_try_dup_anon_rmap_ptes(
+				    folio, page, nr, dst_vma, src_vma))) {
 				folio_ref_sub(folio, nr);
 				return -EAGAIN;
 			}
@@ -1026,11 +1033,13 @@ copy_present_ptes(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma
 		 * guarantee the pinned page won't be randomly replaced in the
 		 * future.
 		 */
-		if (unlikely(folio_try_dup_anon_rmap_pte(folio, page, dst_vma, src_vma))) {
+		if (unlikely(folio_try_dup_anon_rmap_pte(folio, page, dst_vma,
+							 src_vma))) {
 			/* Page may be pinned, we have to copy. */
 			folio_put(folio);
-			err = copy_present_page(dst_vma, src_vma, dst_pte, src_pte,
-						addr, rss, prealloc, page);
+			err = copy_present_page(dst_vma, src_vma, dst_pte,
+						src_pte, addr, rss, prealloc,
+						page);
 			return err ? err : 1;
 		}
 		rss[MM_ANONPAGES]++;
@@ -1046,7 +1055,8 @@ copy_present_ptes(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma
 }
 
 static inline struct folio *folio_prealloc(struct mm_struct *src_mm,
-		struct vm_area_struct *vma, unsigned long addr, bool need_zero)
+					   struct vm_area_struct *vma,
+					   unsigned long addr, bool need_zero)
 {
 	struct folio *new_folio;
 
@@ -1067,10 +1077,9 @@ static inline struct folio *folio_prealloc(struct mm_struct *src_mm,
 	return new_folio;
 }
 
-static int
-copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
-	       pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long addr,
-	       unsigned long end)
+static int copy_pte_range(struct vm_area_struct *dst_vma,
+			  struct vm_area_struct *src_vma, pmd_t *dst_pmd,
+			  pmd_t *src_pmd, unsigned long addr, unsigned long end)
 {
 	struct mm_struct *dst_mm = dst_vma->vm_mm;
 	struct mm_struct *src_mm = src_vma->vm_mm;
@@ -1081,7 +1090,7 @@ copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 	spinlock_t *src_ptl, *dst_ptl;
 	int progress, max_nr, ret = 0;
 	int rss[NR_MM_COUNTERS];
-	swp_entry_t entry = (swp_entry_t){0};
+	swp_entry_t entry = (swp_entry_t){ 0 };
 	struct folio *prealloc = NULL;
 	int nr;
 
@@ -1130,8 +1139,8 @@ copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 		 */
 		if (progress >= 32) {
 			progress = 0;
-			if (need_resched() ||
-			    spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))
+			if (need_resched() || spin_needbreak(src_ptl) ||
+			    spin_needbreak(dst_ptl))
 				break;
 		}
 		ptent = ptep_get(src_pte);
@@ -1140,9 +1149,8 @@ copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 			continue;
 		}
 		if (unlikely(!pte_present(ptent))) {
-			ret = copy_nonpresent_pte(dst_mm, src_mm,
-						  dst_pte, src_pte,
-						  dst_vma, src_vma,
+			ret = copy_nonpresent_pte(dst_mm, src_mm, dst_pte,
+						  src_pte, dst_vma, src_vma,
 						  addr, rss);
 			if (ret == -EIO) {
 				entry = pte_to_swp_entry(ptep_get(src_pte));
@@ -1203,7 +1211,7 @@ copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 		entry.val = 0;
 	} else if (ret == -EBUSY || unlikely(ret == -EHWPOISON)) {
 		goto out;
-	} else if (ret ==  -EAGAIN) {
+	} else if (ret == -EAGAIN) {
 		prealloc = folio_prealloc(src_mm, src_vma, addr, false);
 		if (!prealloc)
 			return -ENOMEM;
@@ -1222,10 +1230,10 @@ copy_pte_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 	return ret;
 }
 
-static inline int
-copy_pmd_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
-	       pud_t *dst_pud, pud_t *src_pud, unsigned long addr,
-	       unsigned long end)
+static inline int copy_pmd_range(struct vm_area_struct *dst_vma,
+				 struct vm_area_struct *src_vma, pud_t *dst_pud,
+				 pud_t *src_pud, unsigned long addr,
+				 unsigned long end)
 {
 	struct mm_struct *dst_mm = dst_vma->vm_mm;
 	struct mm_struct *src_mm = src_vma->vm_mm;
@@ -1238,10 +1246,10 @@ copy_pmd_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 	src_pmd = pmd_offset(src_pud, addr);
 	do {
 		next = pmd_addr_end(addr, end);
-		if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd)
-			|| pmd_devmap(*src_pmd)) {
+		if (is_swap_pmd(*src_pmd) || pmd_trans_huge(*src_pmd) ||
+		    pmd_devmap(*src_pmd)) {
 			int err;
-			VM_BUG_ON_VMA(next-addr != HPAGE_PMD_SIZE, src_vma);
+			VM_BUG_ON_VMA(next - addr != HPAGE_PMD_SIZE, src_vma);
 			err = copy_huge_pmd(dst_mm, src_mm, dst_pmd, src_pmd,
 					    addr, dst_vma, src_vma);
 			if (err == -ENOMEM)
@@ -1252,17 +1260,17 @@ copy_pmd_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 		}
 		if (pmd_none_or_clear_bad(src_pmd))
 			continue;
-		if (copy_pte_range(dst_vma, src_vma, dst_pmd, src_pmd,
-				   addr, next))
+		if (copy_pte_range(dst_vma, src_vma, dst_pmd, src_pmd, addr,
+				   next))
 			return -ENOMEM;
 	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
 	return 0;
 }
 
-static inline int
-copy_pud_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
-	       p4d_t *dst_p4d, p4d_t *src_p4d, unsigned long addr,
-	       unsigned long end)
+static inline int copy_pud_range(struct vm_area_struct *dst_vma,
+				 struct vm_area_struct *src_vma, p4d_t *dst_p4d,
+				 p4d_t *src_p4d, unsigned long addr,
+				 unsigned long end)
 {
 	struct mm_struct *dst_mm = dst_vma->vm_mm;
 	struct mm_struct *src_mm = src_vma->vm_mm;
@@ -1278,9 +1286,9 @@ copy_pud_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 		if (pud_trans_huge(*src_pud) || pud_devmap(*src_pud)) {
 			int err;
 
-			VM_BUG_ON_VMA(next-addr != HPAGE_PUD_SIZE, src_vma);
-			err = copy_huge_pud(dst_mm, src_mm,
-					    dst_pud, src_pud, addr, src_vma);
+			VM_BUG_ON_VMA(next - addr != HPAGE_PUD_SIZE, src_vma);
+			err = copy_huge_pud(dst_mm, src_mm, dst_pud, src_pud,
+					    addr, src_vma);
 			if (err == -ENOMEM)
 				return -ENOMEM;
 			if (!err)
@@ -1289,17 +1297,17 @@ copy_pud_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 		}
 		if (pud_none_or_clear_bad(src_pud))
 			continue;
-		if (copy_pmd_range(dst_vma, src_vma, dst_pud, src_pud,
-				   addr, next))
+		if (copy_pmd_range(dst_vma, src_vma, dst_pud, src_pud, addr,
+				   next))
 			return -ENOMEM;
 	} while (dst_pud++, src_pud++, addr = next, addr != end);
 	return 0;
 }
 
-static inline int
-copy_p4d_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
-	       pgd_t *dst_pgd, pgd_t *src_pgd, unsigned long addr,
-	       unsigned long end)
+static inline int copy_p4d_range(struct vm_area_struct *dst_vma,
+				 struct vm_area_struct *src_vma, pgd_t *dst_pgd,
+				 pgd_t *src_pgd, unsigned long addr,
+				 unsigned long end)
 {
 	struct mm_struct *dst_mm = dst_vma->vm_mm;
 	p4d_t *src_p4d, *dst_p4d;
@@ -1313,8 +1321,8 @@ copy_p4d_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(src_p4d))
 			continue;
-		if (copy_pud_range(dst_vma, src_vma, dst_p4d, src_p4d,
-				   addr, next))
+		if (copy_pud_range(dst_vma, src_vma, dst_p4d, src_p4d, addr,
+				   next))
 			return -ENOMEM;
 	} while (dst_p4d++, src_p4d++, addr = next, addr != end);
 	return 0;
@@ -1325,8 +1333,8 @@ copy_p4d_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma,
  * false when we can speed up fork() by allowing lazy page faults later until
  * when the child accesses the memory range.
  */
-static bool
-vma_needs_copy(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
+static bool vma_needs_copy(struct vm_area_struct *dst_vma,
+			   struct vm_area_struct *src_vma)
 {
 	/*
 	 * Always copy pgtables when dst_vma has uffd-wp enabled even if it's
@@ -1352,8 +1360,8 @@ vma_needs_copy(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
 	return false;
 }
 
-int
-copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
+int copy_page_range(struct vm_area_struct *dst_vma,
+		    struct vm_area_struct *src_vma)
 {
 	pgd_t *src_pgd, *dst_pgd;
 	unsigned long addr = src_vma->vm_start;
@@ -1369,7 +1377,8 @@ copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
 		return 0;
 
 	if (is_vm_hugetlb_page(src_vma))
-		return copy_hugetlb_page_range(dst_mm, src_mm, dst_vma, src_vma);
+		return copy_hugetlb_page_range(dst_mm, src_mm, dst_vma,
+					       src_vma);
 
 	if (unlikely(src_vma->vm_flags & VM_PFNMAP)) {
 		ret = track_pfn_copy(dst_vma, src_vma, &pfn);
@@ -1386,8 +1395,8 @@ copy_page_range(struct vm_area_struct *dst_vma, struct vm_area_struct *src_vma)
 	is_cow = is_cow_mapping(src_vma->vm_flags);
 
 	if (is_cow) {
-		mmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE,
-					0, src_mm, addr, end);
+		mmu_notifier_range_init(&range, MMU_NOTIFY_PROTECTION_PAGE, 0,
+					src_mm, addr, end);
 		mmu_notifier_invalidate_range_start(&range);
 		/*
 		 * Disabling preemption is not needed for the write side, as
@@ -1460,10 +1469,11 @@ static inline bool zap_drop_markers(struct zap_details *details)
  *
  * Returns true if uffd-wp ptes was installed, false otherwise.
  */
-static inline bool
-zap_install_uffd_wp_if_needed(struct vm_area_struct *vma,
-			      unsigned long addr, pte_t *pte, int nr,
-			      struct zap_details *details, pte_t pteval)
+static inline bool zap_install_uffd_wp_if_needed(struct vm_area_struct *vma,
+						 unsigned long addr, pte_t *pte,
+						 int nr,
+						 struct zap_details *details,
+						 pte_t pteval)
 {
 	bool was_installed = false;
 
@@ -1488,11 +1498,12 @@ zap_install_uffd_wp_if_needed(struct vm_area_struct *vma,
 	return was_installed;
 }
 
-static __always_inline void zap_present_folio_ptes(struct mmu_gather *tlb,
-		struct vm_area_struct *vma, struct folio *folio,
-		struct page *page, pte_t *pte, pte_t ptent, unsigned int nr,
-		unsigned long addr, struct zap_details *details, int *rss,
-		bool *force_flush, bool *force_break, bool *any_skipped)
+static __always_inline void
+zap_present_folio_ptes(struct mmu_gather *tlb, struct vm_area_struct *vma,
+		       struct folio *folio, struct page *page, pte_t *pte,
+		       pte_t ptent, unsigned int nr, unsigned long addr,
+		       struct zap_details *details, int *rss, bool *force_flush,
+		       bool *force_break, bool *any_skipped)
 {
 	struct mm_struct *mm = tlb->mm;
 	bool delay_rmap = false;
@@ -1518,8 +1529,8 @@ static __always_inline void zap_present_folio_ptes(struct mmu_gather *tlb,
 	arch_check_zapped_pte(vma, ptent);
 	tlb_remove_tlb_entries(tlb, pte, nr, addr);
 	if (unlikely(userfaultfd_pte_wp(vma, ptent)))
-		*any_skipped = zap_install_uffd_wp_if_needed(vma, addr, pte,
-							     nr, details, ptent);
+		*any_skipped = zap_install_uffd_wp_if_needed(vma, addr, pte, nr,
+							     details, ptent);
 
 	if (!delay_rmap) {
 		folio_remove_rmap_ptes(folio, page, nr, vma);
@@ -1539,11 +1550,11 @@ static __always_inline void zap_present_folio_ptes(struct mmu_gather *tlb,
  *
  * Returns the number of processed (skipped or zapped) PTEs (at least 1).
  */
-static inline int zap_present_ptes(struct mmu_gather *tlb,
-		struct vm_area_struct *vma, pte_t *pte, pte_t ptent,
-		unsigned int max_nr, unsigned long addr,
-		struct zap_details *details, int *rss, bool *force_flush,
-		bool *force_break, bool *any_skipped)
+static inline int
+zap_present_ptes(struct mmu_gather *tlb, struct vm_area_struct *vma, pte_t *pte,
+		 pte_t ptent, unsigned int max_nr, unsigned long addr,
+		 struct zap_details *details, int *rss, bool *force_flush,
+		 bool *force_break, bool *any_skipped)
 {
 	const fpb_t fpb_flags = FPB_IGNORE_DIRTY | FPB_IGNORE_SOFT_DIRTY;
 	struct mm_struct *mm = tlb->mm;
@@ -1558,8 +1569,8 @@ static inline int zap_present_ptes(struct mmu_gather *tlb,
 		arch_check_zapped_pte(vma, ptent);
 		tlb_remove_tlb_entry(tlb, pte, addr);
 		if (userfaultfd_pte_wp(vma, ptent))
-			*any_skipped = zap_install_uffd_wp_if_needed(vma, addr,
-						pte, 1, details, ptent);
+			*any_skipped = zap_install_uffd_wp_if_needed(
+				vma, addr, pte, 1, details, ptent);
 		ksm_might_unmap_zero_page(mm, ptent);
 		return 1;
 	}
@@ -1584,14 +1595,17 @@ static inline int zap_present_ptes(struct mmu_gather *tlb,
 		return nr;
 	}
 	zap_present_folio_ptes(tlb, vma, folio, page, pte, ptent, 1, addr,
-			       details, rss, force_flush, force_break, any_skipped);
+			       details, rss, force_flush, force_break,
+			       any_skipped);
 	return 1;
 }
 
 static inline int zap_nonpresent_ptes(struct mmu_gather *tlb,
-		struct vm_area_struct *vma, pte_t *pte, pte_t ptent,
-		unsigned int max_nr, unsigned long addr,
-		struct zap_details *details, int *rss, bool *any_skipped)
+				      struct vm_area_struct *vma, pte_t *pte,
+				      pte_t ptent, unsigned int max_nr,
+				      unsigned long addr,
+				      struct zap_details *details, int *rss,
+				      bool *any_skipped)
 {
 	swp_entry_t entry;
 	int nr = 1;
@@ -1599,7 +1613,7 @@ static inline int zap_nonpresent_ptes(struct mmu_gather *tlb,
 	*any_skipped = true;
 	entry = pte_to_swp_entry(ptent);
 	if (is_device_private_entry(entry) ||
-		is_device_exclusive_entry(entry)) {
+	    is_device_exclusive_entry(entry)) {
 		struct page *page = pfn_swap_entry_to_page(entry);
 		struct folio *folio = page_folio(page);
 
@@ -1653,7 +1667,8 @@ static inline int zap_nonpresent_ptes(struct mmu_gather *tlb,
 		WARN_ON_ONCE(1);
 	}
 	clear_not_present_full_ptes(vma->vm_mm, addr, pte, nr, tlb->fullmm);
-	*any_skipped = zap_install_uffd_wp_if_needed(vma, addr, pte, nr, details, ptent);
+	*any_skipped = zap_install_uffd_wp_if_needed(vma, addr, pte, nr,
+						     details, ptent);
 
 	return nr;
 }
@@ -1672,6 +1687,10 @@ static inline int do_zap_pte_range(struct mmu_gather *tlb,
 	/* Skip all consecutive none ptes */
 	if (pte_none(ptent)) {
 		for (nr = 1; nr < max_nr; nr++) {
+			/* Added to allow reschedule during large skips */
+			if (need_resched())
+				cond_resched();
+
 			ptent = ptep_get(pte + nr);
 			if (!pte_none(ptent))
 				break;
@@ -1683,6 +1702,10 @@ static inline int do_zap_pte_range(struct mmu_gather *tlb,
 		addr += nr * PAGE_SIZE;
 	}
 
+	/* Added to allow reschedule before processing many PTEs */
+	if (need_resched())
+		cond_resched();
+
 	if (pte_present(ptent))
 		nr += zap_present_ptes(tlb, vma, pte, ptent, max_nr, addr,
 				       details, rss, force_flush, force_break,
@@ -1695,9 +1718,9 @@ static inline int do_zap_pte_range(struct mmu_gather *tlb,
 }
 
 static unsigned long zap_pte_range(struct mmu_gather *tlb,
-				struct vm_area_struct *vma, pmd_t *pmd,
-				unsigned long addr, unsigned long end,
-				struct zap_details *details)
+				   struct vm_area_struct *vma, pmd_t *pmd,
+				   unsigned long addr, unsigned long end,
+				   struct zap_details *details)
 {
 	bool force_flush = false, force_break = false;
 	struct mm_struct *mm = tlb->mm;
@@ -1787,9 +1810,10 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 }
 
 static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,
-				struct vm_area_struct *vma, pud_t *pud,
-				unsigned long addr, unsigned long end,
-				struct zap_details *details)
+					  struct vm_area_struct *vma,
+					  pud_t *pud, unsigned long addr,
+					  unsigned long end,
+					  struct zap_details *details)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -1797,7 +1821,8 @@ static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,
 	pmd = pmd_offset(pud, addr);
 	do {
 		next = pmd_addr_end(addr, end);
-		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
+		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) ||
+		    pmd_devmap(*pmd)) {
 			if (next - addr != HPAGE_PMD_SIZE)
 				__split_huge_pmd(vma, pmd, addr, false, NULL);
 			else if (zap_huge_pmd(tlb, vma, pmd, addr)) {
@@ -1829,9 +1854,10 @@ static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,
 }
 
 static inline unsigned long zap_pud_range(struct mmu_gather *tlb,
-				struct vm_area_struct *vma, p4d_t *p4d,
-				unsigned long addr, unsigned long end,
-				struct zap_details *details)
+					  struct vm_area_struct *vma,
+					  p4d_t *p4d, unsigned long addr,
+					  unsigned long end,
+					  struct zap_details *details)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -1858,9 +1884,10 @@ static inline unsigned long zap_pud_range(struct mmu_gather *tlb,
 }
 
 static inline unsigned long zap_p4d_range(struct mmu_gather *tlb,
-				struct vm_area_struct *vma, pgd_t *pgd,
-				unsigned long addr, unsigned long end,
-				struct zap_details *details)
+					  struct vm_area_struct *vma,
+					  pgd_t *pgd, unsigned long addr,
+					  unsigned long end,
+					  struct zap_details *details)
 {
 	p4d_t *p4d;
 	unsigned long next;
@@ -1876,10 +1903,9 @@ static inline unsigned long zap_p4d_range(struct mmu_gather *tlb,
 	return addr;
 }
 
-void unmap_page_range(struct mmu_gather *tlb,
-			     struct vm_area_struct *vma,
-			     unsigned long addr, unsigned long end,
-			     struct zap_details *details)
+void unmap_page_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
+		      unsigned long addr, unsigned long end,
+		      struct zap_details *details)
 {
 	pgd_t *pgd;
 	unsigned long next;
@@ -1896,11 +1922,9 @@ void unmap_page_range(struct mmu_gather *tlb,
 	tlb_end_vma(tlb, vma);
 }
 
-
-static void unmap_single_vma(struct mmu_gather *tlb,
-		struct vm_area_struct *vma, unsigned long start_addr,
-		unsigned long end_addr,
-		struct zap_details *details, bool mm_wr_locked)
+static void unmap_single_vma(struct mmu_gather *tlb, struct vm_area_struct *vma,
+			     unsigned long start_addr, unsigned long end_addr,
+			     struct zap_details *details, bool mm_wr_locked)
 {
 	unsigned long start = max(vma->vm_start, start_addr);
 	unsigned long end;
@@ -1931,10 +1955,10 @@ static void unmap_single_vma(struct mmu_gather *tlb,
 			 * safe to do nothing in this case.
 			 */
 			if (vma->vm_file) {
-				zap_flags_t zap_flags = details ?
-				    details->zap_flags : 0;
+				zap_flags_t zap_flags =
+					details ? details->zap_flags : 0;
 				__unmap_hugepage_range(tlb, vma, start, end,
-							     NULL, zap_flags);
+						       NULL, zap_flags);
 			}
 		} else
 			unmap_page_range(tlb, vma, start, end, details);
@@ -1981,8 +2005,7 @@ void unmap_vmas(struct mmu_gather *tlb, struct ma_state *mas,
 		unsigned long start = start_addr;
 		unsigned long end = end_addr;
 		hugetlb_zap_begin(vma, &start, &end);
-		unmap_single_vma(tlb, vma, start, end, &details,
-				 mm_wr_locked);
+		unmap_single_vma(tlb, vma, start, end, &details, mm_wr_locked);
 		hugetlb_zap_end(vma, &details);
 		vma = mas_find(mas, tree_end - 1);
 	} while (vma && likely(!xa_is_zero(vma)));
@@ -1999,7 +2022,7 @@ void unmap_vmas(struct mmu_gather *tlb, struct ma_state *mas,
  * The range must fit into one VMA.
  */
 void zap_page_range_single(struct vm_area_struct *vma, unsigned long address,
-		unsigned long size, struct zap_details *details)
+			   unsigned long size, struct zap_details *details)
 {
 	const unsigned long end = address + size;
 	struct mmu_notifier_range range;
@@ -2033,10 +2056,10 @@ void zap_page_range_single(struct vm_area_struct *vma, unsigned long address,
  *
  */
 void zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
-		unsigned long size)
+		  unsigned long size)
 {
 	if (!range_in_vma(vma, address, address + size) ||
-	    		!(vma->vm_flags & VM_PFNMAP))
+	    !(vma->vm_flags & VM_PFNMAP))
 		return;
 
 	zap_page_range_single(vma, address, size, NULL);
@@ -2125,8 +2148,8 @@ static int validate_page_before_insert(struct vm_area_struct *vma,
 }
 
 static int insert_page_into_pte_locked(struct vm_area_struct *vma, pte_t *pte,
-				unsigned long addr, struct page *page,
-				pgprot_t prot, bool mkwrite)
+				       unsigned long addr, struct page *page,
+				       pgprot_t prot, bool mkwrite)
 {
 	struct folio *folio = page_folio(page);
 	pte_t pteval = ptep_get(pte);
@@ -2166,7 +2189,7 @@ static int insert_page_into_pte_locked(struct vm_area_struct *vma, pte_t *pte,
 }
 
 static int insert_page(struct vm_area_struct *vma, unsigned long addr,
-			struct page *page, pgprot_t prot, bool mkwrite)
+		       struct page *page, pgprot_t prot, bool mkwrite)
 {
 	int retval;
 	pte_t *pte;
@@ -2180,14 +2203,15 @@ static int insert_page(struct vm_area_struct *vma, unsigned long addr,
 	if (!pte)
 		goto out;
 	retval = insert_page_into_pte_locked(vma, pte, addr, page, prot,
-					mkwrite);
+					     mkwrite);
 	pte_unmap_unlock(pte, ptl);
 out:
 	return retval;
 }
 
 static int insert_page_in_batch_locked(struct vm_area_struct *vma, pte_t *pte,
-			unsigned long addr, struct page *page, pgprot_t prot)
+				       unsigned long addr, struct page *page,
+				       pgprot_t prot)
 {
 	int err;
 
@@ -2217,8 +2241,8 @@ static int insert_pages(struct vm_area_struct *vma, unsigned long addr,
 	if (!pmd)
 		goto out;
 
-	pages_to_write_in_pmd = min_t(unsigned long,
-		remaining_pages_total, PTRS_PER_PTE - pte_index(addr));
+	pages_to_write_in_pmd = min_t(unsigned long, remaining_pages_total,
+				      PTRS_PER_PTE - pte_index(addr));
 
 	/* Allocate the PTE if necessary; takes PMD lock once only. */
 	ret = -ENOMEM;
@@ -2235,8 +2259,8 @@ static int insert_pages(struct vm_area_struct *vma, unsigned long addr,
 			goto out;
 		}
 		for (pte = start_pte; pte_idx < batch_size; ++pte, ++pte_idx) {
-			int err = insert_page_in_batch_locked(vma, pte,
-				addr, pages[curr_page_idx], prot);
+			int err = insert_page_in_batch_locked(
+				vma, pte, addr, pages[curr_page_idx], prot);
 			if (unlikely(err)) {
 				pte_unmap_unlock(start_pte, pte_lock);
 				ret = err;
@@ -2274,7 +2298,7 @@ static int insert_pages(struct vm_area_struct *vma, unsigned long addr,
  * The same restrictions apply as in vm_insert_page().
  */
 int vm_insert_pages(struct vm_area_struct *vma, unsigned long addr,
-			struct page **pages, unsigned long *num)
+		    struct page **pages, unsigned long *num)
 {
 	const unsigned long end_addr = addr + (*num * PAGE_SIZE) - 1;
 
@@ -2321,7 +2345,7 @@ EXPORT_SYMBOL(vm_insert_pages);
  * Return: %0 on success, negative error code otherwise.
  */
 int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,
-			struct page *page)
+		   struct page *page)
 {
 	if (addr < vma->vm_start || addr >= vma->vm_end)
 		return -EFAULT;
@@ -2348,7 +2372,7 @@ EXPORT_SYMBOL(vm_insert_page);
  * Return: 0 on success and error code otherwise.
  */
 static int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,
-				unsigned long num, unsigned long offset)
+			  unsigned long num, unsigned long offset)
 {
 	unsigned long count = vma_pages(vma);
 	unsigned long uaddr = vma->vm_start;
@@ -2391,7 +2415,7 @@ static int __vm_map_pages(struct vm_area_struct *vma, struct page **pages,
  * Return: 0 on success and error code otherwise.
  */
 int vm_map_pages(struct vm_area_struct *vma, struct page **pages,
-				unsigned long num)
+		 unsigned long num)
 {
 	return __vm_map_pages(vma, pages, num, vma->vm_pgoff);
 }
@@ -2411,14 +2435,14 @@ EXPORT_SYMBOL(vm_map_pages);
  * Return: 0 on success and error code otherwise.
  */
 int vm_map_pages_zero(struct vm_area_struct *vma, struct page **pages,
-				unsigned long num)
+		      unsigned long num)
 {
 	return __vm_map_pages(vma, pages, num, 0);
 }
 EXPORT_SYMBOL(vm_map_pages_zero);
 
 static vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,
-			pfn_t pfn, pgprot_t prot, bool mkwrite)
+			     pfn_t pfn, pgprot_t prot, bool mkwrite)
 {
 	struct mm_struct *mm = vma->vm_mm;
 	pte_t *pte, entry;
@@ -2505,7 +2529,7 @@ static vm_fault_t insert_pfn(struct vm_area_struct *vma, unsigned long addr,
  * Return: vm_fault_t value.
  */
 vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,
-			unsigned long pfn, pgprot_t pgprot)
+			       unsigned long pfn, pgprot_t pgprot)
 {
 	/*
 	 * Technically, architectures with pte_special can avoid all these
@@ -2513,9 +2537,9 @@ vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,
 	 * consistency in testing and feature parity among all, so we should
 	 * try to keep these invariants in place for everybody.
 	 */
-	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
-	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
-						(VM_PFNMAP|VM_MIXEDMAP));
+	BUG_ON(!(vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)));
+	BUG_ON((vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)) ==
+	       (VM_PFNMAP | VM_MIXEDMAP));
 	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
 	BUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));
 
@@ -2528,7 +2552,7 @@ vm_fault_t vmf_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,
 	track_pfn_insert(vma, &pgprot, __pfn_to_pfn_t(pfn, PFN_DEV));
 
 	return insert_pfn(vma, addr, __pfn_to_pfn_t(pfn, PFN_DEV), pgprot,
-			false);
+			  false);
 }
 EXPORT_SYMBOL(vmf_insert_pfn_prot);
 
@@ -2553,7 +2577,7 @@ EXPORT_SYMBOL(vmf_insert_pfn_prot);
  * Return: vm_fault_t value.
  */
 vm_fault_t vmf_insert_pfn(struct vm_area_struct *vma, unsigned long addr,
-			unsigned long pfn)
+			  unsigned long pfn)
 {
 	return vmf_insert_pfn_prot(vma, addr, pfn, vma->vm_page_prot);
 }
@@ -2577,7 +2601,7 @@ static bool vm_mixed_ok(struct vm_area_struct *vma, pfn_t pfn, bool mkwrite)
 }
 
 static vm_fault_t __vm_insert_mixed(struct vm_area_struct *vma,
-		unsigned long addr, pfn_t pfn, bool mkwrite)
+				    unsigned long addr, pfn_t pfn, bool mkwrite)
 {
 	pgprot_t pgprot = vma->vm_page_prot;
 	int err;
@@ -2600,8 +2624,8 @@ static vm_fault_t __vm_insert_mixed(struct vm_area_struct *vma,
 	 * than insert_pfn).  If a zero_pfn were inserted into a VM_MIXEDMAP
 	 * without pte special, it would there be refcounted as a normal page.
 	 */
-	if (!IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL) &&
-	    !pfn_t_devmap(pfn) && pfn_t_valid(pfn)) {
+	if (!IS_ENABLED(CONFIG_ARCH_HAS_PTE_SPECIAL) && !pfn_t_devmap(pfn) &&
+	    pfn_t_valid(pfn)) {
 		struct page *page;
 
 		/*
@@ -2624,7 +2648,7 @@ static vm_fault_t __vm_insert_mixed(struct vm_area_struct *vma,
 }
 
 vm_fault_t vmf_insert_page_mkwrite(struct vm_fault *vmf, struct page *page,
-			bool write)
+				   bool write)
 {
 	pgprot_t pgprot = vmf->vma->vm_page_prot;
 	unsigned long addr = vmf->address;
@@ -2644,7 +2668,7 @@ vm_fault_t vmf_insert_page_mkwrite(struct vm_fault *vmf, struct page *page,
 EXPORT_SYMBOL_GPL(vmf_insert_page_mkwrite);
 
 vm_fault_t vmf_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
-		pfn_t pfn)
+			    pfn_t pfn)
 {
 	return __vm_insert_mixed(vma, addr, pfn, false);
 }
@@ -2656,7 +2680,7 @@ EXPORT_SYMBOL(vmf_insert_mixed);
  *  the same entry was actually inserted.
  */
 vm_fault_t vmf_insert_mixed_mkwrite(struct vm_area_struct *vma,
-		unsigned long addr, pfn_t pfn)
+				    unsigned long addr, pfn_t pfn)
 {
 	return __vm_insert_mixed(vma, addr, pfn, true);
 }
@@ -2666,9 +2690,8 @@ vm_fault_t vmf_insert_mixed_mkwrite(struct vm_area_struct *vma,
  * mappings are removed. any references to nonexistent pages results
  * in null mappings (currently treated as "copy-on-access")
  */
-static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
-			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot)
+static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd, unsigned long addr,
+			   unsigned long end, unsigned long pfn, pgprot_t prot)
 {
 	pte_t *pte, *mapped_pte;
 	spinlock_t *ptl;
@@ -2693,8 +2716,8 @@ static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 }
 
 static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
-			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot)
+				  unsigned long addr, unsigned long end,
+				  unsigned long pfn, pgprot_t prot)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -2708,7 +2731,7 @@ static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
 	do {
 		next = pmd_addr_end(addr, end);
 		err = remap_pte_range(mm, pmd, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot);
+				      pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (pmd++, addr = next, addr != end);
@@ -2716,8 +2739,8 @@ static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
 }
 
 static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,
-			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot)
+				  unsigned long addr, unsigned long end,
+				  unsigned long pfn, pgprot_t prot)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -2730,7 +2753,7 @@ static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,
 	do {
 		next = pud_addr_end(addr, end);
 		err = remap_pmd_range(mm, pud, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot);
+				      pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (pud++, addr = next, addr != end);
@@ -2738,8 +2761,8 @@ static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,
 }
 
 static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,
-			unsigned long addr, unsigned long end,
-			unsigned long pfn, pgprot_t prot)
+				  unsigned long addr, unsigned long end,
+				  unsigned long pfn, pgprot_t prot)
 {
 	p4d_t *p4d;
 	unsigned long next;
@@ -2752,15 +2775,16 @@ static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,
 	do {
 		next = p4d_addr_end(addr, end);
 		err = remap_pud_range(mm, p4d, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot);
+				      pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (p4d++, addr = next, addr != end);
 	return 0;
 }
 
-static int remap_pfn_range_internal(struct vm_area_struct *vma, unsigned long addr,
-		unsigned long pfn, unsigned long size, pgprot_t prot)
+static int remap_pfn_range_internal(struct vm_area_struct *vma,
+				    unsigned long addr, unsigned long pfn,
+				    unsigned long size, pgprot_t prot)
 {
 	pgd_t *pgd;
 	unsigned long next;
@@ -2804,7 +2828,7 @@ static int remap_pfn_range_internal(struct vm_area_struct *vma, unsigned long ad
 	do {
 		next = pgd_addr_end(addr, end);
 		err = remap_p4d_range(mm, pgd, addr, next,
-				pfn + (addr >> PAGE_SHIFT), prot);
+				      pfn + (addr >> PAGE_SHIFT), prot);
 		if (err)
 			return err;
 	} while (pgd++, addr = next, addr != end);
@@ -2817,7 +2841,8 @@ static int remap_pfn_range_internal(struct vm_area_struct *vma, unsigned long ad
  * must have pre-validated the caching bits of the pgprot_t.
  */
 int remap_pfn_range_notrack(struct vm_area_struct *vma, unsigned long addr,
-		unsigned long pfn, unsigned long size, pgprot_t prot)
+			    unsigned long pfn, unsigned long size,
+			    pgprot_t prot)
 {
 	int error = remap_pfn_range_internal(vma, addr, pfn, size, prot);
 
@@ -2876,7 +2901,8 @@ EXPORT_SYMBOL(remap_pfn_range);
  *
  * Return: %0 on success, negative error code otherwise.
  */
-int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)
+int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start,
+		    unsigned long len)
 {
 	unsigned long vm_len, pfn, pages;
 
@@ -2906,29 +2932,32 @@ int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long
 		return -EINVAL;
 
 	/* Ok, let it rip */
-	return io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);
+	return io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len,
+				  vma->vm_page_prot);
 }
 EXPORT_SYMBOL(vm_iomap_memory);
 
 static int apply_to_pte_range(struct mm_struct *mm, pmd_t *pmd,
-				     unsigned long addr, unsigned long end,
-				     pte_fn_t fn, void *data, bool create,
-				     pgtbl_mod_mask *mask)
+			      unsigned long addr, unsigned long end,
+			      pte_fn_t fn, void *data, bool create,
+			      pgtbl_mod_mask *mask)
 {
 	pte_t *pte, *mapped_pte;
 	int err = 0;
 	spinlock_t *ptl;
 
 	if (create) {
-		mapped_pte = pte = (mm == &init_mm) ?
-			pte_alloc_kernel_track(pmd, addr, mask) :
-			pte_alloc_map_lock(mm, pmd, addr, &ptl);
+		mapped_pte = pte =
+			(mm == &init_mm) ?
+				pte_alloc_kernel_track(pmd, addr, mask) :
+				pte_alloc_map_lock(mm, pmd, addr, &ptl);
 		if (!pte)
 			return -ENOMEM;
 	} else {
-		mapped_pte = pte = (mm == &init_mm) ?
-			pte_offset_kernel(pmd, addr) :
-			pte_offset_map_lock(mm, pmd, addr, &ptl);
+		mapped_pte = pte =
+			(mm == &init_mm) ?
+				pte_offset_kernel(pmd, addr) :
+				pte_offset_map_lock(mm, pmd, addr, &ptl);
 		if (!pte)
 			return -EINVAL;
 	}
@@ -2954,9 +2983,9 @@ static int apply_to_pte_range(struct mm_struct *mm, pmd_t *pmd,
 }
 
 static int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,
-				     unsigned long addr, unsigned long end,
-				     pte_fn_t fn, void *data, bool create,
-				     pgtbl_mod_mask *mask)
+			      unsigned long addr, unsigned long end,
+			      pte_fn_t fn, void *data, bool create,
+			      pgtbl_mod_mask *mask)
 {
 	pmd_t *pmd;
 	unsigned long next;
@@ -2982,8 +3011,8 @@ static int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,
 				continue;
 			pmd_clear_bad(pmd);
 		}
-		err = apply_to_pte_range(mm, pmd, addr, next,
-					 fn, data, create, mask);
+		err = apply_to_pte_range(mm, pmd, addr, next, fn, data, create,
+					 mask);
 		if (err)
 			break;
 	} while (pmd++, addr = next, addr != end);
@@ -2992,9 +3021,9 @@ static int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,
 }
 
 static int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,
-				     unsigned long addr, unsigned long end,
-				     pte_fn_t fn, void *data, bool create,
-				     pgtbl_mod_mask *mask)
+			      unsigned long addr, unsigned long end,
+			      pte_fn_t fn, void *data, bool create,
+			      pgtbl_mod_mask *mask)
 {
 	pud_t *pud;
 	unsigned long next;
@@ -3018,8 +3047,8 @@ static int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,
 				continue;
 			pud_clear_bad(pud);
 		}
-		err = apply_to_pmd_range(mm, pud, addr, next,
-					 fn, data, create, mask);
+		err = apply_to_pmd_range(mm, pud, addr, next, fn, data, create,
+					 mask);
 		if (err)
 			break;
 	} while (pud++, addr = next, addr != end);
@@ -3028,9 +3057,9 @@ static int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,
 }
 
 static int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,
-				     unsigned long addr, unsigned long end,
-				     pte_fn_t fn, void *data, bool create,
-				     pgtbl_mod_mask *mask)
+			      unsigned long addr, unsigned long end,
+			      pte_fn_t fn, void *data, bool create,
+			      pgtbl_mod_mask *mask)
 {
 	p4d_t *p4d;
 	unsigned long next;
@@ -3054,8 +3083,8 @@ static int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,
 				continue;
 			p4d_clear_bad(p4d);
 		}
-		err = apply_to_pud_range(mm, p4d, addr, next,
-					 fn, data, create, mask);
+		err = apply_to_pud_range(mm, p4d, addr, next, fn, data, create,
+					 mask);
 		if (err)
 			break;
 	} while (p4d++, addr = next, addr != end);
@@ -3064,8 +3093,8 @@ static int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,
 }
 
 static int __apply_to_page_range(struct mm_struct *mm, unsigned long addr,
-				 unsigned long size, pte_fn_t fn,
-				 void *data, bool create)
+				 unsigned long size, pte_fn_t fn, void *data,
+				 bool create)
 {
 	pgd_t *pgd;
 	unsigned long start = addr, next;
@@ -3090,8 +3119,8 @@ static int __apply_to_page_range(struct mm_struct *mm, unsigned long addr,
 				continue;
 			pgd_clear_bad(pgd);
 		}
-		err = apply_to_p4d_range(mm, pgd, addr, next,
-					 fn, data, create, &mask);
+		err = apply_to_p4d_range(mm, pgd, addr, next, fn, data, create,
+					 &mask);
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
@@ -3190,7 +3219,8 @@ static inline int __wp_page_copy_user(struct page *dst, struct page *src,
 		pte_t entry;
 
 		vmf->pte = pte_offset_map_lock(mm, vmf->pmd, addr, &vmf->ptl);
-		if (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {
+		if (unlikely(!vmf->pte ||
+			     !pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {
 			/*
 			 * Other thread has already handled the fault
 			 * and update local tlb only
@@ -3218,7 +3248,8 @@ static inline int __wp_page_copy_user(struct page *dst, struct page *src,
 
 		/* Re-validate under PTL if the page is still mapped */
 		vmf->pte = pte_offset_map_lock(mm, vmf->pmd, addr, &vmf->ptl);
-		if (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {
+		if (unlikely(!vmf->pte ||
+			     !pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {
 			/* The PTE changed under us, update local tlb */
 			if (vmf->pte)
 				update_mmu_tlb(vma, addr, vmf->pte);
@@ -3258,7 +3289,8 @@ static gfp_t __get_fault_gfp_mask(struct vm_area_struct *vma)
 	struct file *vm_file = vma->vm_file;
 
 	if (vm_file)
-		return mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS | __GFP_IO;
+		return mapping_gfp_mask(vm_file->f_mapping) | __GFP_FS |
+		       __GFP_IO;
 
 	/*
 	 * Special mappings (e.g. VDSO) do not have any file so fake
@@ -3278,7 +3310,7 @@ static vm_fault_t do_page_mkwrite(struct vm_fault *vmf, struct folio *folio)
 	vm_fault_t ret;
 	unsigned int old_flags = vmf->flags;
 
-	vmf->flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
+	vmf->flags = FAULT_FLAG_WRITE | FAULT_FLAG_MKWRITE;
 
 	if (vmf->vma->vm_file &&
 	    IS_SWAPFILE(vmf->vma->vm_file->f_mapping->host))
@@ -3542,7 +3574,8 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		 * some TLBs while the old PTE remains in others.
 		 */
 		ptep_clear_flush(vma, vmf->address, vmf->pte);
-		folio_add_new_anon_rmap(new_folio, vma, vmf->address, RMAP_EXCLUSIVE);
+		folio_add_new_anon_rmap(new_folio, vma, vmf->address,
+					RMAP_EXCLUSIVE);
 		folio_add_lru_vma(new_folio, vma);
 		BUG_ON(unshare && pte_write(entry));
 		set_pte_at(mm, vmf->address, vmf->pte, entry);
@@ -3621,7 +3654,8 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
  * Return: %0 on success, %VM_FAULT_NOPAGE when PTE got changed before
  * we acquired PTE lock.
  */
-static vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf, struct folio *folio)
+static vm_fault_t finish_mkwrite_fault(struct vm_fault *vmf,
+				       struct folio *folio)
 {
 	WARN_ON_ONCE(!(vmf->vma->vm_flags & VM_SHARED));
 	vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address,
@@ -3686,8 +3720,8 @@ static vm_fault_t wp_page_shared(struct vm_fault *vmf, struct folio *folio)
 		}
 
 		tmp = do_page_mkwrite(vmf, folio);
-		if (unlikely(!tmp || (tmp &
-				      (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
+		if (unlikely(!tmp ||
+			     (tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
 			folio_put(folio);
 			return tmp;
 		}
@@ -3709,7 +3743,7 @@ static vm_fault_t wp_page_shared(struct vm_fault *vmf, struct folio *folio)
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static bool __wp_can_reuse_large_anon_folio(struct folio *folio,
-		struct vm_area_struct *vma)
+					    struct vm_area_struct *vma)
 {
 	bool exclusive = false;
 
@@ -3758,7 +3792,8 @@ static bool __wp_can_reuse_large_anon_folio(struct folio *folio,
 	if (folio_large_mapcount(folio) != folio_ref_count(folio))
 		goto unlock;
 
-	VM_WARN_ON_ONCE_FOLIO(folio_large_mapcount(folio) > folio_nr_pages(folio), folio);
+	VM_WARN_ON_ONCE_FOLIO(
+		folio_large_mapcount(folio) > folio_nr_pages(folio), folio);
 	VM_WARN_ON_ONCE_FOLIO(folio_entire_mapcount(folio), folio);
 	VM_WARN_ON_ONCE(folio_mm_id(folio, 0) != vma->vm_mm->mm_id &&
 			folio_mm_id(folio, 1) != vma->vm_mm->mm_id);
@@ -3775,7 +3810,7 @@ static bool __wp_can_reuse_large_anon_folio(struct folio *folio,
 }
 #else /* !CONFIG_TRANSPARENT_HUGEPAGE */
 static bool __wp_can_reuse_large_anon_folio(struct folio *folio,
-		struct vm_area_struct *vma)
+					    struct vm_area_struct *vma)
 {
 	BUILD_BUG();
 }
@@ -3844,8 +3879,7 @@ static bool wp_can_reuse_anon_folio(struct folio *folio,
  * but allow concurrent faults), with pte both mapped and locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_wp_page(struct vm_fault *vmf)
-	__releases(vmf->ptl)
+static vm_fault_t do_wp_page(struct vm_fault *vmf) __releases(vmf->ptl)
 {
 	const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
 	struct vm_area_struct *vma = vmf->vma;
@@ -3915,7 +3949,8 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	 * the page without further checks.
 	 */
 	if (folio && folio_test_anon(folio) &&
-	    (PageAnonExclusive(vmf->page) || wp_can_reuse_anon_folio(folio, vma))) {
+	    (PageAnonExclusive(vmf->page) ||
+	     wp_can_reuse_anon_folio(folio, vma))) {
 		if (!PageAnonExclusive(vmf->page))
 			SetPageAnonExclusive(vmf->page);
 		if (unlikely(unshare)) {
@@ -3940,8 +3975,9 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 }
 
 static void unmap_mapping_range_vma(struct vm_area_struct *vma,
-		unsigned long start_addr, unsigned long end_addr,
-		struct zap_details *details)
+				    unsigned long start_addr,
+				    unsigned long end_addr,
+				    struct zap_details *details)
 {
 	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
 }
@@ -3954,16 +3990,17 @@ static inline void unmap_mapping_range_tree(struct rb_root_cached *root,
 	struct vm_area_struct *vma;
 	pgoff_t vba, vea, zba, zea;
 
-	vma_interval_tree_foreach(vma, root, first_index, last_index) {
+	vma_interval_tree_foreach(vma, root, first_index, last_index)
+	{
 		vba = vma->vm_pgoff;
 		vea = vba + vma_pages(vma) - 1;
 		zba = max(first_index, vba);
 		zea = min(last_index, vea);
 
-		unmap_mapping_range_vma(vma,
-			((zba - vba) << PAGE_SHIFT) + vma->vm_start,
+		unmap_mapping_range_vma(
+			vma, ((zba - vba) << PAGE_SHIFT) + vma->vm_start,
 			((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
-				details);
+			details);
 	}
 }
 
@@ -3981,9 +4018,9 @@ static inline void unmap_mapping_range_tree(struct rb_root_cached *root,
 void unmap_mapping_folio(struct folio *folio)
 {
 	struct address_space *mapping = folio->mapping;
-	struct zap_details details = { };
-	pgoff_t	first_index;
-	pgoff_t	last_index;
+	struct zap_details details = {};
+	pgoff_t first_index;
+	pgoff_t last_index;
 
 	VM_BUG_ON(!folio_test_locked(folio));
 
@@ -4014,11 +4051,11 @@ void unmap_mapping_folio(struct folio *folio)
  * cache.
  */
 void unmap_mapping_pages(struct address_space *mapping, pgoff_t start,
-		pgoff_t nr, bool even_cows)
+			 pgoff_t nr, bool even_cows)
 {
-	struct zap_details details = { };
-	pgoff_t	first_index = start;
-	pgoff_t	last_index = start + nr - 1;
+	struct zap_details details = {};
+	pgoff_t first_index = start;
+	pgoff_t last_index = start + nr - 1;
 
 	details.even_cows = even_cows;
 	if (last_index < first_index)
@@ -4049,16 +4086,16 @@ EXPORT_SYMBOL_GPL(unmap_mapping_pages);
  * @even_cows: 1 when truncating a file, unmap even private COWed pages;
  * but 0 when invalidating pagecache, don't throw away private data.
  */
-void unmap_mapping_range(struct address_space *mapping,
-		loff_t const holebegin, loff_t const holelen, int even_cows)
+void unmap_mapping_range(struct address_space *mapping, loff_t const holebegin,
+			 loff_t const holelen, int even_cows)
 {
 	pgoff_t hba = (pgoff_t)(holebegin) >> PAGE_SHIFT;
 	pgoff_t hlen = ((pgoff_t)(holelen) + PAGE_SIZE - 1) >> PAGE_SHIFT;
 
 	/* Check for overflow. */
 	if (sizeof(holelen) > sizeof(hlen)) {
-		long long holeend =
-			(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
+		long long holeend = (holebegin + holelen + PAGE_SIZE - 1) >>
+				    PAGE_SHIFT;
 		if (holeend & ~(long long)ULONG_MAX)
 			hlen = ULONG_MAX - hba + 1;
 	}
@@ -4093,13 +4130,14 @@ static vm_fault_t remove_device_exclusive_entry(struct vm_fault *vmf)
 		folio_put(folio);
 		return ret;
 	}
-	mmu_notifier_range_init_owner(&range, MMU_NOTIFY_CLEAR, 0,
-				vma->vm_mm, vmf->address & PAGE_MASK,
-				(vmf->address & PAGE_MASK) + PAGE_SIZE, NULL);
+	mmu_notifier_range_init_owner(&range, MMU_NOTIFY_CLEAR, 0, vma->vm_mm,
+				      vmf->address & PAGE_MASK,
+				      (vmf->address & PAGE_MASK) + PAGE_SIZE,
+				      NULL);
 	mmu_notifier_invalidate_range_start(&range);
 
 	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
-				&vmf->ptl);
+				       &vmf->ptl);
 	if (likely(vmf->pte && pte_same(ptep_get(vmf->pte), vmf->orig_pte)))
 		restore_exclusive_pte(vma, folio, vmf->page, vmf->address,
 				      vmf->pte, vmf->orig_pte);
@@ -4129,13 +4167,13 @@ static inline bool should_try_to_free_swap(struct folio *folio,
 	 * reference only in case it's likely that we'll be the exlusive user.
 	 */
 	return (fault_flags & FAULT_FLAG_WRITE) && !folio_test_ksm(folio) &&
-		folio_ref_count(folio) == (1 + folio_nr_pages(folio));
+	       folio_ref_count(folio) == (1 + folio_nr_pages(folio));
 }
 
 static vm_fault_t pte_marker_clear(struct vm_fault *vmf)
 {
-	vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd,
-				       vmf->address, &vmf->ptl);
+	vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address,
+				       &vmf->ptl);
 	if (!vmf->pte)
 		return 0;
 	/*
@@ -4214,8 +4252,8 @@ static struct folio *__alloc_swap_folio(struct vm_fault *vmf)
 		return NULL;
 
 	entry = pte_to_swp_entry(vmf->orig_pte);
-	if (mem_cgroup_swapin_charge_folio(folio, vma->vm_mm,
-					   GFP_KERNEL, entry)) {
+	if (mem_cgroup_swapin_charge_folio(folio, vma->vm_mm, GFP_KERNEL,
+					   entry)) {
 		folio_put(folio);
 		return NULL;
 	}
@@ -4334,10 +4372,11 @@ static struct folio *alloc_swap_folio(struct vm_fault *vmf)
 	 * and suitable for swapping THP.
 	 */
 	orders = thp_vma_allowable_orders(vma, vma->vm_flags,
-			TVA_IN_PF | TVA_ENFORCE_SYSFS, BIT(PMD_ORDER) - 1);
+					  TVA_IN_PF | TVA_ENFORCE_SYSFS,
+					  BIT(PMD_ORDER) - 1);
 	orders = thp_vma_suitable_orders(vma, vmf->address, orders);
-	orders = thp_swap_suitable_orders(swp_offset(entry),
-					  vmf->address, orders);
+	orders = thp_swap_suitable_orders(swp_offset(entry), vmf->address,
+					  orders);
 
 	if (!orders)
 		goto fallback;
@@ -4440,10 +4479,9 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 
 			vmf->page = pfn_swap_entry_to_page(entry);
 			vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
-					vmf->address, &vmf->ptl);
-			if (unlikely(!vmf->pte ||
-				     !pte_same(ptep_get(vmf->pte),
-							vmf->orig_pte)))
+						       vmf->address, &vmf->ptl);
+			if (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte),
+							    vmf->orig_pte)))
 				goto unlock;
 
 			/*
@@ -4494,7 +4532,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 
 				nr_pages = folio_nr_pages(folio);
 				if (folio_test_large(folio))
-					entry.val = ALIGN_DOWN(entry.val, nr_pages);
+					entry.val =
+						ALIGN_DOWN(entry.val, nr_pages);
 				/*
 				 * Prevent parallel swapin from proceeding with
 				 * the cache flag. Otherwise, another thread
@@ -4530,7 +4569,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 			}
 		} else {
 			folio = swapin_readahead(entry, GFP_HIGHUSER_MOVABLE,
-						vmf);
+						 vmf);
 			swapcache = folio;
 		}
 
@@ -4540,7 +4579,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 			 * while we released the pte lock.
 			 */
 			vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
-					vmf->address, &vmf->ptl);
+						       vmf->address, &vmf->ptl);
 			if (likely(vmf->pte &&
 				   pte_same(ptep_get(vmf->pte), vmf->orig_pte)))
 				ret = VM_FAULT_OOM;
@@ -4612,7 +4651,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	 * Back out if somebody else already faulted in this pte.
 	 */
 	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
-			&vmf->ptl);
+				       &vmf->ptl);
 	if (unlikely(!vmf->pte || !pte_same(ptep_get(vmf->pte), vmf->orig_pte)))
 		goto out_nomap;
 
@@ -4624,12 +4663,14 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	/* allocated large folios for SWP_SYNCHRONOUS_IO */
 	if (folio_test_large(folio) && !folio_test_swapcache(folio)) {
 		unsigned long nr = folio_nr_pages(folio);
-		unsigned long folio_start = ALIGN_DOWN(vmf->address, nr * PAGE_SIZE);
+		unsigned long folio_start =
+			ALIGN_DOWN(vmf->address, nr * PAGE_SIZE);
 		unsigned long idx = (vmf->address - folio_start) / PAGE_SIZE;
 		pte_t *folio_ptep = vmf->pte - idx;
 		pte_t folio_pte = ptep_get(folio_ptep);
 
-		if (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||
+		if (!pte_same(folio_pte,
+			      pte_move_swp_offset(vmf->orig_pte, -idx)) ||
 		    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)
 			goto out_nomap;
 
@@ -4651,14 +4692,16 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 		pte_t *folio_ptep;
 		pte_t folio_pte;
 
-		if (unlikely(folio_start < max(address & PMD_MASK, vma->vm_start)))
+		if (unlikely(folio_start <
+			     max(address & PMD_MASK, vma->vm_start)))
 			goto check_folio;
 		if (unlikely(folio_end > pmd_addr_end(address, vma->vm_end)))
 			goto check_folio;
 
 		folio_ptep = vmf->pte - idx;
 		folio_pte = ptep_get(folio_ptep);
-		if (!pte_same(folio_pte, pte_move_swp_offset(vmf->orig_pte, -idx)) ||
+		if (!pte_same(folio_pte,
+			      pte_move_swp_offset(vmf->orig_pte, -idx)) ||
 		    swap_pte_batch(folio_ptep, nr, folio_pte) != nr)
 			goto check_folio;
 
@@ -4695,7 +4738,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 			 */
 			exclusive = true;
 		} else if (exclusive && folio_test_writeback(folio) &&
-			  data_race(si->flags & SWP_STABLE_WRITES)) {
+			   data_race(si->flags & SWP_STABLE_WRITES)) {
 			/*
 			 * This is tricky: not all swap backends support
 			 * concurrent page modifications while under writeback.
@@ -4750,7 +4793,8 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	 */
 	if (!folio_test_ksm(folio) &&
 	    (exclusive || folio_ref_count(folio) == 1)) {
-		if ((vma->vm_flags & VM_WRITE) && !userfaultfd_pte_wp(vma, pte) &&
+		if ((vma->vm_flags & VM_WRITE) &&
+		    !userfaultfd_pte_wp(vma, pte) &&
 		    !pte_needs_soft_dirty_wp(vma, pte)) {
 			pte = pte_mkwrite(pte, vma);
 			if (vmf->flags & FAULT_FLAG_WRITE) {
@@ -4775,19 +4819,19 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 		 * folios which are fully exclusive. If we ever get large
 		 * folios within swapcache here, we have to be careful.
 		 */
-		VM_WARN_ON_ONCE(folio_test_large(folio) && folio_test_swapcache(folio));
+		VM_WARN_ON_ONCE(folio_test_large(folio) &&
+				folio_test_swapcache(folio));
 		VM_WARN_ON_FOLIO(!folio_test_locked(folio), folio);
 		folio_add_new_anon_rmap(folio, vma, address, rmap_flags);
 	} else {
 		folio_add_anon_rmap_ptes(folio, page, nr_pages, vma, address,
-					rmap_flags);
+					 rmap_flags);
 	}
 
 	VM_BUG_ON(!folio_test_anon(folio) ||
-			(pte_write(pte) && !PageAnonExclusive(page)));
+		  (pte_write(pte) && !PageAnonExclusive(page)));
 	set_ptes(vma->vm_mm, address, ptep, pte, nr_pages);
-	arch_do_swap_page_nr(vma->vm_mm, vma, address,
-			pte, pte, nr_pages);
+	arch_do_swap_page_nr(vma->vm_mm, vma, address, pte, pte, nr_pages);
 
 	folio_unlock(folio);
 	if (folio != swapcache && swapcache) {
@@ -4882,7 +4926,8 @@ static struct folio *alloc_anon_folio(struct vm_fault *vmf)
 	 * the faulting address and still be fully contained in the vma.
 	 */
 	orders = thp_vma_allowable_orders(vma, vma->vm_flags,
-			TVA_IN_PF | TVA_ENFORCE_SYSFS, BIT(PMD_ORDER) - 1);
+					  TVA_IN_PF | TVA_ENFORCE_SYSFS,
+					  BIT(PMD_ORDER) - 1);
 	orders = thp_vma_suitable_orders(vma, vmf->address, orders);
 
 	if (!orders)
@@ -4917,7 +4962,9 @@ static struct folio *alloc_anon_folio(struct vm_fault *vmf)
 		folio = vma_alloc_folio(gfp, order, vma, addr);
 		if (folio) {
 			if (mem_cgroup_charge(folio, vma->vm_mm, gfp)) {
-				count_mthp_stat(order, MTHP_STAT_ANON_FAULT_FALLBACK_CHARGE);
+				count_mthp_stat(
+					order,
+					MTHP_STAT_ANON_FAULT_FALLBACK_CHARGE);
 				folio_put(folio);
 				goto next;
 			}
@@ -4970,11 +5017,11 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 
 	/* Use the zero-page for reads */
 	if (!(vmf->flags & FAULT_FLAG_WRITE) &&
-			!mm_forbids_zeropage(vma->vm_mm)) {
-		entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf->address),
-						vma->vm_page_prot));
+	    !mm_forbids_zeropage(vma->vm_mm)) {
+		entry = pte_mkspecial(
+			pfn_pte(my_zero_pfn(vmf->address), vma->vm_page_prot));
 		vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
-				vmf->address, &vmf->ptl);
+					       vmf->address, &vmf->ptl);
 		if (!vmf->pte)
 			goto unlock;
 		if (vmf_pte_changed(vmf)) {
@@ -5227,8 +5274,8 @@ vm_fault_t do_set_pmd(struct vm_fault *vmf, struct page *page)
  * @nr: The number of PTEs to create.
  * @addr: The first address to create a PTE for.
  */
-void set_pte_range(struct vm_fault *vmf, struct folio *folio,
-		struct page *page, unsigned int nr, unsigned long addr)
+void set_pte_range(struct vm_fault *vmf, struct folio *folio, struct page *page,
+		   unsigned int nr, unsigned long addr)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	bool write = vmf->flags & FAULT_FLAG_WRITE;
@@ -5351,9 +5398,9 @@ vm_fault_t finish_fault(struct vm_fault *vmf)
 		 * cache beyond the VMA limits and PMD pagetable limits.
 		 */
 		if (unlikely(vma_off < idx ||
-			    vma_off + (nr_pages - idx) > vma_pages(vma) ||
-			    pte_off < idx ||
-			    pte_off + (nr_pages - idx)  > PTRS_PER_PTE)) {
+			     vma_off + (nr_pages - idx) > vma_pages(vma) ||
+			     pte_off < idx ||
+			     pte_off + (nr_pages - idx) > PTRS_PER_PTE)) {
 			nr_pages = 1;
 		} else {
 			/* Now we can set mappings for the whole large folio. */
@@ -5362,8 +5409,7 @@ vm_fault_t finish_fault(struct vm_fault *vmf)
 		}
 	}
 
-	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
-				       addr, &vmf->ptl);
+	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, addr, &vmf->ptl);
 	if (!vmf->pte)
 		return VM_FAULT_NOPAGE;
 
@@ -5389,8 +5435,7 @@ vm_fault_t finish_fault(struct vm_fault *vmf)
 	return ret;
 }
 
-static unsigned long fault_around_pages __read_mostly =
-	65536 >> PAGE_SHIFT;
+static unsigned long fault_around_pages __read_mostly = 65536 >> PAGE_SHIFT;
 
 #ifdef CONFIG_DEBUG_FS
 static int fault_around_bytes_get(void *data, u64 *val)
@@ -5417,8 +5462,8 @@ static int fault_around_bytes_set(void *data, u64 val)
 
 	return 0;
 }
-DEFINE_DEBUGFS_ATTRIBUTE(fault_around_bytes_fops,
-		fault_around_bytes_get, fault_around_bytes_set, "%llu\n");
+DEFINE_DEBUGFS_ATTRIBUTE(fault_around_bytes_fops, fault_around_bytes_get,
+			 fault_around_bytes_set, "%llu\n");
 
 static int __init fault_around_debugfs(void)
 {
@@ -5464,7 +5509,8 @@ static vm_fault_t do_fault_around(struct vm_fault *vmf)
 
 	/* The PTE offset of the end address, clamped to the VMA and PTE. */
 	to_pte = min3(from_pte + nr_pages, (pgoff_t)PTRS_PER_PTE,
-		      pte_off + vma_pages(vmf->vma) - vma_off) - 1;
+		      pte_off + vma_pages(vmf->vma) - vma_off) -
+		 1;
 
 	if (pmd_none(*vmf->pmd)) {
 		vmf->prealloc_pte = pte_alloc_one(vmf->vma->vm_mm);
@@ -5473,9 +5519,8 @@ static vm_fault_t do_fault_around(struct vm_fault *vmf)
 	}
 
 	rcu_read_lock();
-	ret = vmf->vma->vm_ops->map_pages(vmf,
-			vmf->pgoff + from_pte - pte_off,
-			vmf->pgoff + to_pte - pte_off);
+	ret = vmf->vma->vm_ops->map_pages(vmf, vmf->pgoff + from_pte - pte_off,
+					  vmf->pgoff + to_pte - pte_off);
 	rcu_read_unlock();
 
 	return ret;
@@ -5551,7 +5596,8 @@ static vm_fault_t do_cow_fault(struct vm_fault *vmf)
 	if (ret & VM_FAULT_DONE_COW)
 		return ret;
 
-	if (copy_mc_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma)) {
+	if (copy_mc_user_highpage(vmf->cow_page, vmf->page, vmf->address,
+				  vma)) {
 		ret = VM_FAULT_HWPOISON;
 		goto unlock;
 	}
@@ -5593,15 +5639,15 @@ static vm_fault_t do_shared_fault(struct vm_fault *vmf)
 		folio_unlock(folio);
 		tmp = do_page_mkwrite(vmf, folio);
 		if (unlikely(!tmp ||
-				(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
+			     (tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
 			folio_put(folio);
 			return tmp;
 		}
 	}
 
 	ret |= finish_fault(vmf);
-	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
-					VM_FAULT_RETRY))) {
+	if (unlikely(ret &
+		     (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY))) {
 		folio_unlock(folio);
 		folio_put(folio);
 		return ret;
@@ -5664,8 +5710,8 @@ static vm_fault_t do_fault(struct vm_fault *vmf)
 }
 
 int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,
-		      unsigned long addr, int *flags,
-		      bool writable, int *last_cpupid)
+		       unsigned long addr, int *flags, bool writable,
+		       int *last_cpupid)
 {
 	struct vm_area_struct *vma = vmf->vma;
 
@@ -5710,9 +5756,10 @@ int numa_migrate_check(struct folio *folio, struct vm_fault *vmf,
 	return mpol_misplaced(folio, vmf, addr);
 }
 
-static void numa_rebuild_single_mapping(struct vm_fault *vmf, struct vm_area_struct *vma,
-					unsigned long fault_addr, pte_t *fault_pte,
-					bool writable)
+static void numa_rebuild_single_mapping(struct vm_fault *vmf,
+					struct vm_area_struct *vma,
+					unsigned long fault_addr,
+					pte_t *fault_pte, bool writable)
 {
 	pte_t pte, old_pte;
 
@@ -5725,9 +5772,11 @@ static void numa_rebuild_single_mapping(struct vm_fault *vmf, struct vm_area_str
 	update_mmu_cache_range(vmf, vma, fault_addr, fault_pte, 1);
 }
 
-static void numa_rebuild_large_mapping(struct vm_fault *vmf, struct vm_area_struct *vma,
+static void numa_rebuild_large_mapping(struct vm_fault *vmf,
+				       struct vm_area_struct *vma,
 				       struct folio *folio, pte_t fault_pte,
-				       bool ignore_writable, bool pte_write_upgrade)
+				       bool ignore_writable,
+				       bool pte_write_upgrade)
 {
 	int nr = pte_pfn(fault_pte) - folio_pfn(folio);
 	unsigned long start, end, addr = vmf->address;
@@ -5760,7 +5809,8 @@ static void numa_rebuild_large_mapping(struct vm_fault *vmf, struct vm_area_stru
 				writable = true;
 		}
 
-		numa_rebuild_single_mapping(vmf, vma, addr, start_ptep, writable);
+		numa_rebuild_single_mapping(vmf, vma, addr, start_ptep,
+					    writable);
 	}
 }
 
@@ -5829,8 +5879,8 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 	}
 
 	flags |= TNF_MIGRATE_FAIL;
-	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
-				       vmf->address, &vmf->ptl);
+	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
+				       &vmf->ptl);
 	if (unlikely(!vmf->pte))
 		return 0;
 	if (unlikely(!pte_same(ptep_get(vmf->pte), vmf->orig_pte))) {
@@ -5843,8 +5893,8 @@ static vm_fault_t do_numa_page(struct vm_fault *vmf)
 	 * non-accessible ptes, some can allow access by kernel mode.
 	 */
 	if (folio && folio_test_large(folio))
-		numa_rebuild_large_mapping(vmf, vma, folio, pte, ignore_writable,
-					   pte_write_upgrade);
+		numa_rebuild_large_mapping(vmf, vma, folio, pte,
+					   ignore_writable, pte_write_upgrade);
 	else
 		numa_rebuild_single_mapping(vmf, vma, vmf->address, vmf->pte,
 					    writable);
@@ -5899,7 +5949,7 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)
 
 static vm_fault_t create_huge_pud(struct vm_fault *vmf)
 {
-#if defined(CONFIG_TRANSPARENT_HUGEPAGE) &&			\
+#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && \
 	defined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)
 	struct vm_area_struct *vma = vmf->vma;
 	/* No support for anonymous transparent PUD pages yet */
@@ -5913,7 +5963,7 @@ static vm_fault_t create_huge_pud(struct vm_fault *vmf)
 
 static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
 {
-#if defined(CONFIG_TRANSPARENT_HUGEPAGE) &&			\
+#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && \
 	defined(CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD)
 	struct vm_area_struct *vma = vmf->vma;
 	vm_fault_t ret;
@@ -6008,7 +6058,7 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 		update_mmu_tlb(vmf->vma, vmf->address, vmf->pte);
 		goto unlock;
 	}
-	if (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {
+	if (vmf->flags & (FAULT_FLAG_WRITE | FAULT_FLAG_UNSHARE)) {
 		if (!pte_write(entry))
 			return do_wp_page(vmf);
 		else if (likely(vmf->flags & FAULT_FLAG_WRITE))
@@ -6016,9 +6066,9 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	}
 	entry = pte_mkyoung(entry);
 	if (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,
-				vmf->flags & FAULT_FLAG_WRITE)) {
-		update_mmu_cache_range(vmf, vmf->vma, vmf->address,
-				vmf->pte, 1);
+				  vmf->flags & FAULT_FLAG_WRITE)) {
+		update_mmu_cache_range(vmf, vmf->vma, vmf->address, vmf->pte,
+				       1);
 	} else {
 		/* Skip spurious TLB flush for retried page fault */
 		if (vmf->flags & FAULT_FLAG_TRIED)
@@ -6045,7 +6095,7 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * and __folio_lock_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+				    unsigned long address, unsigned int flags)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -6072,7 +6122,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 retry_pud:
 	if (pud_none(*vmf.pud) &&
 	    thp_vma_allowable_order(vma, vm_flags,
-				TVA_IN_PF | TVA_ENFORCE_SYSFS, PUD_ORDER)) {
+				    TVA_IN_PF | TVA_ENFORCE_SYSFS, PUD_ORDER)) {
 		ret = create_huge_pud(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
 			return ret;
@@ -6081,12 +6131,12 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 
 		barrier();
 		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {
-
 			/*
 			 * TODO once we support anonymous PUDs: NUMA case and
 			 * FAULT_FLAG_UNSHARE handling.
 			 */
-			if ((flags & FAULT_FLAG_WRITE) && !pud_write(orig_pud)) {
+			if ((flags & FAULT_FLAG_WRITE) &&
+			    !pud_write(orig_pud)) {
 				ret = wp_huge_pud(&vmf, orig_pud);
 				if (!(ret & VM_FAULT_FALLBACK))
 					return ret;
@@ -6107,7 +6157,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 
 	if (pmd_none(*vmf.pmd) &&
 	    thp_vma_allowable_order(vma, vm_flags,
-				TVA_IN_PF | TVA_ENFORCE_SYSFS, PMD_ORDER)) {
+				    TVA_IN_PF | TVA_ENFORCE_SYSFS, PMD_ORDER)) {
 		ret = create_huge_pmd(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
 			return ret;
@@ -6116,16 +6166,17 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 
 		if (unlikely(is_swap_pmd(vmf.orig_pmd))) {
 			VM_BUG_ON(thp_migration_supported() &&
-					  !is_pmd_migration_entry(vmf.orig_pmd));
+				  !is_pmd_migration_entry(vmf.orig_pmd));
 			if (is_pmd_migration_entry(vmf.orig_pmd))
 				pmd_migration_entry_wait(mm, vmf.pmd);
 			return 0;
 		}
 		if (pmd_trans_huge(vmf.orig_pmd) || pmd_devmap(vmf.orig_pmd)) {
-			if (pmd_protnone(vmf.orig_pmd) && vma_is_accessible(vma))
+			if (pmd_protnone(vmf.orig_pmd) &&
+			    vma_is_accessible(vma))
 				return do_huge_pmd_numa_page(&vmf);
 
-			if ((flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) &&
+			if ((flags & (FAULT_FLAG_WRITE | FAULT_FLAG_UNSHARE)) &&
 			    !pmd_write(vmf.orig_pmd)) {
 				ret = wp_huge_pmd(&vmf);
 				if (!(ret & VM_FAULT_FALLBACK))
@@ -6256,8 +6307,8 @@ static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,
 	 * the assumption that lock is dropped on VM_FAULT_RETRY.
 	 */
 	if (WARN_ON_ONCE((*flags &
-			(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)) ==
-			(FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)))
+			  (FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)) ==
+			 (FAULT_FLAG_VMA_LOCK | FAULT_FLAG_RETRY_NOWAIT)))
 		return VM_FAULT_SIGSEGV;
 #endif
 
@@ -6286,8 +6337,8 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		goto out;
 
 	if (!arch_vma_access_permitted(vma, flags & FAULT_FLAG_WRITE,
-					    flags & FAULT_FLAG_INSTRUCTION,
-					    flags & FAULT_FLAG_REMOTE)) {
+				       flags & FAULT_FLAG_INSTRUCTION,
+				       flags & FAULT_FLAG_REMOTE)) {
 		ret = VM_FAULT_SIGSEGV;
 		goto out;
 	}
@@ -6341,7 +6392,8 @@ EXPORT_SYMBOL_GPL(handle_mm_fault);
 #ifdef CONFIG_LOCK_MM_AND_FIND_VMA
 #include <linux/extable.h>
 
-static inline bool get_mmap_lock_carefully(struct mm_struct *mm, struct pt_regs *regs)
+static inline bool get_mmap_lock_carefully(struct mm_struct *mm,
+					   struct pt_regs *regs)
 {
 	if (likely(mmap_read_trylock(mm)))
 		return true;
@@ -6368,7 +6420,8 @@ static inline bool mmap_upgrade_trylock(struct mm_struct *mm)
 	return false;
 }
 
-static inline bool upgrade_mmap_lock_carefully(struct mm_struct *mm, struct pt_regs *regs)
+static inline bool upgrade_mmap_lock_carefully(struct mm_struct *mm,
+					       struct pt_regs *regs)
 {
 	mmap_read_unlock(mm);
 	if (regs && !user_mode(regs)) {
@@ -6399,7 +6452,8 @@ static inline bool upgrade_mmap_lock_carefully(struct mm_struct *mm, struct pt_r
  * need to extend the vma, which helps the VM layer a lot.
  */
 struct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,
-			unsigned long addr, struct pt_regs *regs)
+					    unsigned long addr,
+					    struct pt_regs *regs)
 {
 	struct vm_area_struct *vma;
 
@@ -6455,7 +6509,8 @@ struct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,
 #endif
 
 #ifdef CONFIG_PER_VMA_LOCK
-static inline bool __vma_enter_locked(struct vm_area_struct *vma, bool detaching)
+static inline bool __vma_enter_locked(struct vm_area_struct *vma,
+				      bool detaching)
 {
 	unsigned int tgt_refcnt = VMA_LOCK_OFFSET;
 
@@ -6472,8 +6527,8 @@ static inline bool __vma_enter_locked(struct vm_area_struct *vma, bool detaching
 
 	rwsem_acquire(&vma->vmlock_dep_map, 0, 0, _RET_IP_);
 	rcuwait_wait_event(&vma->vm_mm->vma_writer_wait,
-		   refcount_read(&vma->vm_refcnt) == tgt_refcnt,
-		   TASK_UNINTERRUPTIBLE);
+			   refcount_read(&vma->vm_refcnt) == tgt_refcnt,
+			   TASK_UNINTERRUPTIBLE);
 	lock_acquired(&vma->vmlock_dep_map, _RET_IP_);
 
 	return true;
@@ -6574,8 +6629,8 @@ struct vm_area_struct *lock_vma_under_rcu(struct mm_struct *mm,
 	 */
 
 	/* Check if the vma we locked is the right one. */
-	if (unlikely(vma->vm_mm != mm ||
-		     address < vma->vm_start || address >= vma->vm_end))
+	if (unlikely(vma->vm_mm != mm || address < vma->vm_start ||
+		     address >= vma->vm_end))
 		goto inval_end_read;
 
 	rcu_read_unlock();
@@ -6602,7 +6657,7 @@ int __p4d_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
 		return -ENOMEM;
 
 	spin_lock(&mm->page_table_lock);
-	if (pgd_present(*pgd)) {	/* Another has populated it */
+	if (pgd_present(*pgd)) { /* Another has populated it */
 		p4d_free(mm, new);
 	} else {
 		smp_wmb(); /* See comment in pmd_install() */
@@ -6629,7 +6684,7 @@ int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)
 		mm_inc_nr_puds(mm);
 		smp_wmb(); /* See comment in pmd_install() */
 		p4d_populate(mm, p4d, new);
-	} else	/* Another has populated it */
+	} else /* Another has populated it */
 		pud_free(mm, new);
 	spin_unlock(&mm->page_table_lock);
 	return 0;
@@ -6653,7 +6708,7 @@ int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
 		mm_inc_nr_pmds(mm);
 		smp_wmb(); /* See comment in pmd_install() */
 		pud_populate(mm, pud, new);
-	} else {	/* Another has populated it */
+	} else { /* Another has populated it */
 		pmd_free(mm, new);
 	}
 	spin_unlock(ptl);
@@ -6786,9 +6841,8 @@ int follow_pfnmap_start(struct follow_pfnmap_args *args)
 	pte = ptep_get(ptep);
 	if (!pte_present(pte))
 		goto unlock;
-	pfnmap_args_setup(args, lock, ptep, pte_pgprot(pte),
-			  pte_pfn(pte), PAGE_MASK, pte_write(pte),
-			  pte_special(pte));
+	pfnmap_args_setup(args, lock, ptep, pte_pgprot(pte), pte_pfn(pte),
+			  PAGE_MASK, pte_write(pte), pte_special(pte));
 	return 0;
 unlock:
 	pte_unmap_unlock(ptep, lock);
@@ -6901,8 +6955,8 @@ static int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
 		int bytes, offset;
 		void *maddr;
 		struct vm_area_struct *vma = NULL;
-		struct page *page = get_user_page_vma_remote(mm, addr,
-							     gup_flags, &vma);
+		struct page *page =
+			get_user_page_vma_remote(mm, addr, gup_flags, &vma);
 
 		if (IS_ERR(page)) {
 			/* We might need to expand the stack to access it */
@@ -6925,16 +6979,16 @@ static int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
 			bytes = 0;
 #ifdef CONFIG_HAVE_IOREMAP_PROT
 			if (vma->vm_ops && vma->vm_ops->access)
-				bytes = vma->vm_ops->access(vma, addr, buf,
-							    len, write);
+				bytes = vma->vm_ops->access(vma, addr, buf, len,
+							    write);
 #endif
 			if (bytes <= 0)
 				break;
 		} else {
 			bytes = len;
-			offset = addr & (PAGE_SIZE-1);
-			if (bytes > PAGE_SIZE-offset)
-				bytes = PAGE_SIZE-offset;
+			offset = addr & (PAGE_SIZE - 1);
+			if (bytes > PAGE_SIZE - offset)
+				bytes = PAGE_SIZE - offset;
 
 			maddr = kmap_local_page(page);
 			if (write) {
@@ -6942,8 +6996,8 @@ static int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
 						  maddr + offset, buf, bytes);
 				set_page_dirty_lock(page);
 			} else {
-				copy_from_user_page(vma, page, addr,
-						    buf, maddr + offset, bytes);
+				copy_from_user_page(vma, page, addr, buf,
+						    maddr + offset, bytes);
 			}
 			unmap_and_put_page(page, maddr);
 		}
@@ -6968,8 +7022,8 @@ static int __access_remote_vm(struct mm_struct *mm, unsigned long addr,
  *
  * Return: number of bytes copied from source to destination.
  */
-int access_remote_vm(struct mm_struct *mm, unsigned long addr,
-		void *buf, int len, unsigned int gup_flags)
+int access_remote_vm(struct mm_struct *mm, unsigned long addr, void *buf,
+		     int len, unsigned int gup_flags)
 {
 	return __access_remote_vm(mm, addr, buf, len, gup_flags);
 }
@@ -6979,8 +7033,8 @@ int access_remote_vm(struct mm_struct *mm, unsigned long addr,
  * Source/target buffer must be kernel space,
  * Do not walk the page table directly, use get_user_pages
  */
-int access_process_vm(struct task_struct *tsk, unsigned long addr,
-		void *buf, int len, unsigned int gup_flags)
+int access_process_vm(struct task_struct *tsk, unsigned long addr, void *buf,
+		      int len, unsigned int gup_flags)
 {
 	struct mm_struct *mm;
 	int ret;
@@ -7061,7 +7115,8 @@ static int __copy_remote_vm_str(struct mm_struct *mm, unsigned long addr,
 		 */
 		if (bytes != len) {
 			addr += bytes - 1;
-			copy_from_user_page(vma, page, addr, buf, maddr + (PAGE_SIZE - 1), 1);
+			copy_from_user_page(vma, page, addr, buf,
+					    maddr + (PAGE_SIZE - 1), 1);
 			buf += 1;
 			addr += 1;
 		}
@@ -7091,8 +7146,8 @@ static int __copy_remote_vm_str(struct mm_struct *mm, unsigned long addr,
  * not including the trailing NUL. Always guaranteed to leave NUL-terminated
  * buffer. On any error, return -EFAULT.
  */
-int copy_remote_vm_str(struct task_struct *tsk, unsigned long addr,
-		       void *buf, int len, unsigned int gup_flags)
+int copy_remote_vm_str(struct task_struct *tsk, unsigned long addr, void *buf,
+		       int len, unsigned int gup_flags)
 {
 	struct mm_struct *mm;
 	int ret;
@@ -7134,9 +7189,8 @@ void print_vma_addr(char *prefix, unsigned long ip)
 		struct file *f = vma->vm_file;
 		ip -= vma->vm_start;
 		ip += vma->vm_pgoff << PAGE_SHIFT;
-		printk("%s%pD[%lx,%lx+%lx]", prefix, f, ip,
-				vma->vm_start,
-				vma->vm_end - vma->vm_start);
+		printk("%s%pD[%lx,%lx+%lx]", prefix, f, ip, vma->vm_start,
+		       vma->vm_end - vma->vm_start);
 	}
 	mmap_read_unlock(mm);
 }
@@ -7159,14 +7213,15 @@ EXPORT_SYMBOL(__might_fault);
  * operation.  The target subpage will be processed last to keep its
  * cache lines hot.
  */
-static inline int process_huge_page(
-	unsigned long addr_hint, unsigned int nr_pages,
-	int (*process_subpage)(unsigned long addr, int idx, void *arg),
-	void *arg)
+static inline int process_huge_page(unsigned long addr_hint,
+				    unsigned int nr_pages,
+				    int (*process_subpage)(unsigned long addr,
+							   int idx, void *arg),
+				    void *arg)
 {
 	int i, n, base, l, ret;
 	unsigned long addr = addr_hint &
-		~(((unsigned long)nr_pages << PAGE_SHIFT) - 1);
+			     ~(((unsigned long)nr_pages << PAGE_SHIFT) - 1);
 
 	/* Process target subpage last to keep its cache lines hot */
 	might_sleep();
@@ -7203,11 +7258,13 @@ static inline int process_huge_page(
 		int right_idx = base + 2 * l - 1 - i;
 
 		cond_resched();
-		ret = process_subpage(addr + left_idx * PAGE_SIZE, left_idx, arg);
+		ret = process_subpage(addr + left_idx * PAGE_SIZE, left_idx,
+				      arg);
 		if (ret)
 			return ret;
 		cond_resched();
-		ret = process_subpage(addr + right_idx * PAGE_SIZE, right_idx, arg);
+		ret = process_subpage(addr + right_idx * PAGE_SIZE, right_idx,
+				      arg);
 		if (ret)
 			return ret;
 	}
@@ -7266,7 +7323,7 @@ static int copy_user_gigantic_page(struct folio *dst, struct folio *src,
 
 		cond_resched();
 		if (copy_mc_user_highpage(dst_page, src_page,
-					  addr + i*PAGE_SIZE, vma))
+					  addr + i * PAGE_SIZE, vma))
 			return -EHWPOISON;
 	}
 	return 0;
@@ -7300,14 +7357,14 @@ int copy_user_large_folio(struct folio *dst, struct folio *src,
 	};
 
 	if (unlikely(nr_pages > MAX_ORDER_NR_PAGES))
-		return copy_user_gigantic_page(dst, src, addr_hint, vma, nr_pages);
+		return copy_user_gigantic_page(dst, src, addr_hint, vma,
+					       nr_pages);
 
 	return process_huge_page(addr_hint, nr_pages, copy_subpage, &arg);
 }
 
-long copy_folio_from_user(struct folio *dst_folio,
-			   const void __user *usr_src,
-			   bool allow_pagefault)
+long copy_folio_from_user(struct folio *dst_folio, const void __user *usr_src,
+			  bool allow_pagefault)
 {
 	void *kaddr;
 	unsigned long i, rc = 0;
@@ -7344,7 +7401,7 @@ static struct kmem_cache *page_ptl_cachep;
 void __init ptlock_cache_init(void)
 {
 	page_ptl_cachep = kmem_cache_create("page->ptl", sizeof(spinlock_t), 0,
-			SLAB_PANIC, NULL);
+					    SLAB_PANIC, NULL);
 }
 
 bool ptlock_alloc(struct ptdesc *ptdesc)
-- 
2.43.0

